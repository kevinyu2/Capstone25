{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_sparse(net_file,ngene):\n",
    "    ppi_df = pd.read_csv(net_file,header=None,sep='\\t')\n",
    "    A = np.zeros((ngene,ngene))\n",
    "    row_idx = ppi_df.iloc[:,0].values -1 \n",
    "    col_idx = ppi_df.iloc[:,1].values -1\n",
    "    A[row_idx, col_idx] = ppi_df.iloc[:,2].values\n",
    "    assert (A == A.T).all()\n",
    "    zero_rows = np.all(A == 0, axis=1)\n",
    "    diag_indices = np.arange(ngene)\n",
    "    A[diag_indices[zero_rows], diag_indices[zero_rows]] = 1\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_nets(ppi_files,n_gene):\n",
    "    '''\n",
    "    parameters:\n",
    "    - ppi_files: [str, str, ...], list of network file paths, each file should contain three columns: [protein1, protein2, score]\n",
    "    - ref_gene_file: str, file path, the file contains all genes, one gene per line\n",
    "    output:\n",
    "    - nets: n_file x n_gene x n_gene array with ppi networks\n",
    "    '''\n",
    "    n_file = len(ppi_files)\n",
    "    nets = np.zeros((n_file,n_gene,n_gene))\n",
    "    for i in range(n_file):\n",
    "        A = load_network_sparse(ppi_files[i],n_gene)\n",
    "        nets[i,:,:] = A\n",
    "    return nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rwr_original_sparse(ppi_files,restart_prob,ngene,nets):\n",
    "    ''' \n",
    "    - ppi_files: list of network file paths\n",
    "    - restart_prob: RWR restart probability\n",
    "    - ngene: number of genes\n",
    "    output:\n",
    "    - walks: for the i-th RWR result, [i,:,:], each column is the stationary distribution of a node\n",
    "    '''\n",
    "    n_file = len(ppi_files)\n",
    "    e = np.ones(ngene)\n",
    "    I = np.eye(ngene)\n",
    "    walks = np.zeros((n_file,ngene,ngene))\n",
    "    for i in range(n_file):\n",
    "        A = nets[i,:,:]\n",
    "        d = A @ e\n",
    "        P = A / d # transition matrix\n",
    "        W = (I - (1 - restart_prob) * P)\n",
    "        W = np.linalg.inv(W)\n",
    "        W = W * restart_prob \n",
    "        walks[i,:,:] = W\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_embed_sparse_func(walks, ngene, embed_dim):\n",
    "    n_net = walks.shape[0]\n",
    "    mat = np.zeros((ngene,ngene))\n",
    "    W_updated = np.zeros_like(walks)\n",
    "    for i in range(n_net):\n",
    "        W = walks[i,:,:]\n",
    "        W[W<=1e-8] = 0\n",
    "        W = np.log(W, where = W > 1e-8)\n",
    "        W_updated[i,:,:] = W\n",
    "        tmp = W.T @ W\n",
    "        mat = mat + tmp\n",
    "    eigenvalues, eigenvectors = scipy.sparse.linalg.eigs(mat,k=embed_dim)\n",
    "    x = np.diag(np.sqrt(np.sqrt(eigenvalues))) @ eigenvectors.T\n",
    "    return np.real(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test_anno(rand,fold,org,ont_type,ont_size1,ont_size2):\n",
    "    '''\n",
    "    predifined fold splits\n",
    "    - rand: 1 2 3 4 5\n",
    "    - fold: 1 2 3 4 5\n",
    "    - org: \"Ecoli\" or \"yeast\"\n",
    "    '''\n",
    "    file_name = 'data/train_test_split/'+org+'/rand' + str(rand) +'/fold' + str(fold) + '_' + ont_type+ '_' +  str(ont_size1)+ '_' +  str(ont_size2)+ '_train_anno.txt'\n",
    "    train = pd.read_csv(file_name,header=None,sep = '\\t')\n",
    "    file_name = 'data/train_test_split/'+org+'/rand' + str(rand) +'/fold' + str(fold) + '_' + ont_type+ '_' +  str(ont_size1)+ '_' +  str(ont_size2)+ '_test_anno.txt'\n",
    "    test = pd.read_csv(file_name,header=None,sep = '\\t')\n",
    "    return train.to_numpy(), test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_graph(nets, ngene, gene_clusters, mustlink_weight, cannotlink_weight):\n",
    "    '''\n",
    "    - nets: original adjacency matrices directly read from PPI files\n",
    "    - gene_clusters: (num_clusers, num_genes), binary matrix indicating which gene belongs to which clusters\n",
    "    '''\n",
    "    n_nets = nets.shape[0]\n",
    "    n_clusters = gene_clusters.shape[0]\n",
    "    augmented = np.zeros((n_nets,(ngene+n_clusters),(ngene+n_clusters)))\n",
    "    for i in range(n_nets):\n",
    "        A = nets[i,:,:]\n",
    "        A_block = np.block([[A,mustlink_weight*gene_clusters.T],[mustlink_weight*gene_clusters,cannotlink_weight*np.ones((n_clusters,n_clusters))]])\n",
    "        np.fill_diagonal(A_block,0)\n",
    "        zero_rows = np.all(np.absolute(A_block) == 0, axis=1)\n",
    "        diag_indices = np.arange(ngene+n_clusters)\n",
    "        A_block[diag_indices[zero_rows], diag_indices[zero_rows]] = 1\n",
    "        augmented[i,:,:] = A_block\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_RWR(augmented_nets, restart_prob):\n",
    "    '''\n",
    "    RWR for augmented graph which contains negative edge weights\n",
    "    '''\n",
    "    n_nets = augmented_nets.shape[0]\n",
    "    n_nodes = augmented_nets.shape[1]\n",
    "    augmented_walks = np.zeros((n_nets,n_nodes,n_nodes))\n",
    "    e = np.ones(n_nodes)\n",
    "    for i in range(n_nets):\n",
    "        A = augmented_nets[i,:,:]\n",
    "        d = np.absolute(A) @ e\n",
    "        L = np.diag(d) - (1-restart_prob)*A\n",
    "        L_inv = np.linalg.inv(L)\n",
    "        W = restart_prob*(np.diag(d) @ L_inv)\n",
    "        augmented_walks[i,:,:] = W\n",
    "    return augmented_walks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_SVD_with_cannolink(aug_walks, embed_dim):\n",
    "    n_net = aug_walks.shape[0]\n",
    "    n_node = aug_walks.shape[1]\n",
    "    mat = np.zeros((n_node,n_node))\n",
    "    W_updated = np.zeros_like(aug_walks)\n",
    "    for i in range(n_net):\n",
    "        W = aug_walks[i,:,:]\n",
    "        min_entry = W.min()\n",
    "        if min_entry > 0:\n",
    "            min_entry = 0.0\n",
    "        W = W - min_entry\n",
    "        W[W<=1e-8] = 0\n",
    "        W = np.log(W, where = W > 1e-8)\n",
    "        W_updated[i,:,:] = W\n",
    "        tmp = W.T @ W\n",
    "        mat = mat + tmp\n",
    "    eigenvalues, eigenvectors = scipy.sparse.linalg.eigs(mat,k=embed_dim)\n",
    "    x = np.diag(np.sqrt(np.sqrt(eigenvalues))) @ eigenvectors.T\n",
    "    return np.real(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_knn_ind(embed,train_anno):\n",
    "    '''\n",
    "    parameters:\n",
    "    - embed: (dim, num_gene), protein embeddings\n",
    "    - train_anno: annotations for training proteins\n",
    "    output:\n",
    "    dist_mat: n_gene x n_gene\n",
    "    sorted_ind: ngene x (ngene-1), top n labels\n",
    "    '''\n",
    "    n_gene = train_anno.shape[1]\n",
    "    dist_mat = squareform(pdist(embed.T))\n",
    "    dist_mat = dist_mat[:n_gene,:n_gene] # symmetrical\n",
    "    np.fill_diagonal(dist_mat, 1e8)\n",
    "    sorted_ind = np.argsort(dist_mat, axis=1)\n",
    "    \n",
    "    return dist_mat, sorted_ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(dist_mat, knn_mat, train_anno, test_anno,k, weighted=True):\n",
    "    '''\n",
    "    parameters:\n",
    "    - dist_mat: n_gene x n_gene\n",
    "    - knn_mat: ngene x (ngene-1), sorted labels\n",
    "    - train_anno: n_label x n_gene\n",
    "    - test_anno: n_label x n_gene\n",
    "    - k: number of nearest neighbors\n",
    "    - weighted: boolean, whether doing weighted majority vote or not\n",
    "    output:\n",
    "    - final_scores: n_label x n_test, normalized scores of each label\n",
    "    - num_voters: vector of numbers of voting nodes\n",
    "    '''\n",
    "    train_idx = np.where(sum(train_anno)>0)[0]\n",
    "    test_idx = np.where(sum(test_anno)>0)[0]\n",
    "    final_scores = np.zeros((train_anno.shape[0],len(test_idx)))\n",
    "    num_voters = []\n",
    "    updated_voters = []\n",
    "    c = 0\n",
    "    for index, i in enumerate(test_idx):\n",
    "        nn = knn_mat[i,:k]\n",
    "        nn_labeled = nn[np.isin(nn, train_idx)] \n",
    "        \n",
    "        if len(nn_labeled) == 0: # if within the first k neighbors, no neighbor is labeled, then use the nearest neighbor with label\n",
    "            voting_node = knn_mat[i,:][np.isin(knn_mat[i,:], train_idx)][0]\n",
    "            scores = np.array(train_anno[:,voting_node])\n",
    "            scores = scores / sum(scores)\n",
    "            num_voters.append(len(nn_labeled))\n",
    "            tmp = [voting_node]\n",
    "            updated_voters.append(tmp)\n",
    "        else:\n",
    "            votes = np.array(train_anno[:,nn_labeled])\n",
    "            if weighted:\n",
    "                d = dist_mat[i,nn_labeled]\n",
    "                d = d[np.nonzero(d)]\n",
    "                votes = np.array(train_anno[:,nn_labeled[np.nonzero(d)]])\n",
    "                tmp = nn_labeled[np.nonzero(d)]\n",
    "                updated_voters.append(tmp)\n",
    "                num_voters.append(len(d))\n",
    "                if len(d) == 0:\n",
    "                    c += 1\n",
    "                    voting_node = np.random.choice(train_idx)\n",
    "                    scores = np.array(train_anno[:,voting_node])\n",
    "                    scores = scores / sum(scores)\n",
    "                else:\n",
    "                    weights = 1 / d\n",
    "                    scores = votes @ weights.T\n",
    "                    scores = scores / sum(scores)\n",
    "            else:\n",
    "                num_voters.append(len(nn_labeled))\n",
    "                updated_voters.append(nn_labeled)\n",
    "                scores = np.sum(votes,axis=1)\n",
    "                scores = scores / sum(scores)\n",
    "        \n",
    "        final_scores[:,index] = np.squeeze(scores)\n",
    "        # print(c)\n",
    "    return final_scores, num_voters,updated_voters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_top1_pred(test_scores, test_anno):\n",
    "    '''\n",
    "    for each test gene, find the label with the highest predicted score, use it as the predicted label\n",
    "    accuracy is defined as (#predicted label in test true labels) / (#test genes)\n",
    "    problems: if there's a tie, the one with smaller index will be used\n",
    "    parameters:\n",
    "    - test_scores: n_label x n_test\n",
    "    - test_anno: n_label x n_gene\n",
    "    output:\n",
    "    - acc: accuracy score\n",
    "    '''\n",
    "    test_idx = np.where(sum(test_anno)>0)[0]\n",
    "    zero_idx = np.where(np.sum(test_scores,axis=0)==0)[0]\n",
    "    mask = np.ones(len(test_idx), dtype=bool)\n",
    "    mask[zero_idx] = False\n",
    "    test_anno = test_anno[:,test_idx] # n_label x n_test\n",
    "    sorted_index = np.argsort(-1*test_scores,axis=0) # n_label x n_test, with row 0 the highest predicted label for each gene\n",
    "    true_pred = test_anno[sorted_index[0,:], np.arange(test_anno.shape[1])]\n",
    "    true_pred = true_pred[mask]\n",
    "    acc = np.mean(true_pred)\n",
    "    return acc,true_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_auprc_pred(test_scores, test_anno,top_n):\n",
    "    '''\n",
    "    for each test gene, find the labels with the top_n highest predicted scores, use them as the predicted labels\n",
    "    f1 is defined as 2*TP / 2*TP + FP + FN\n",
    "    probelms: if there's a tie, the one with smaller index will be used, only top n predictions will be considered, it will increase the number of FN\n",
    "    parameters:\n",
    "    - test_scores: n_label x n_test\n",
    "    - test_anno: n_label x n_gene\n",
    "    - top_n: int, the number of labels to be predicted\n",
    "    output:\n",
    "    - acc: accuracy score\n",
    "    '''\n",
    "    test_idx = np.where(sum(test_anno)>0)[0]\n",
    "    zero_idx = np.where(np.sum(test_scores,axis=0)==0)[0]\n",
    "\n",
    "    mask = np.ones(len(test_idx), dtype=bool)\n",
    "    mask[zero_idx] = False\n",
    "    test_anno = test_anno[:,test_idx] # n_label x n_test\n",
    "    \n",
    "    test_anno = test_anno[:,mask]\n",
    "    test_scores = test_scores[:,mask]\n",
    "    sorted_index = np.argsort(-1*test_scores,axis=0) # n_label x n_test, with row 0 the highest predicted label for each gene\n",
    "    top_ind = sorted_index[:top_n,:].flatten()\n",
    "    pred = np.zeros_like(test_anno)\n",
    "    cols = np.tile(np.arange(test_anno.shape[1]), top_n)\n",
    "    pred[top_ind, cols] = 1\n",
    "    f1 = sklearn.metrics.f1_score(test_anno.flatten(),pred.flatten())\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(test_anno.flatten(), pred.flatten())\n",
    "    auprc = sklearn.metrics.auc(recall, precision)\n",
    "    return f1, auprc\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for each augmented node, randomly choose a fixed number of nodes to connect to\n",
    "the number of genes that each augmented node connects to are the same except for the last one\n",
    "'''\n",
    "def random_split_vector(train_anno,n_gene, num_sub_vectors,seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    input_vector = np.where(sum(train_anno)>0)[0]\n",
    "    \n",
    "    if num_sub_vectors <= 0 or num_sub_vectors > len(input_vector):\n",
    "        raise ValueError(\"Invalid number of sub-vectors\")\n",
    "    \n",
    "    shuffled_vector = np.random.permutation(input_vector)\n",
    "    sub_vector_size = len(shuffled_vector) // num_sub_vectors\n",
    "    \n",
    "    group_matrix = np.zeros((num_sub_vectors, len(input_vector)), dtype=int)\n",
    "    res_matrix = np.zeros((num_sub_vectors, n_gene), dtype=int)\n",
    "    \n",
    "    start_index = 0\n",
    "    for i in range(num_sub_vectors):\n",
    "        end_index = start_index + sub_vector_size\n",
    "        \n",
    "        if i == num_sub_vectors - 1:\n",
    "            end_index = len(shuffled_vector)\n",
    "        \n",
    "        selected_indices = shuffled_vector[start_index:end_index] \n",
    "        group_matrix[i, np.isin(shuffled_vector, selected_indices)] = 1\n",
    "        \n",
    "        start_index = end_index\n",
    "    \n",
    "    res_matrix[:,shuffled_vector] = group_matrix\n",
    "    \n",
    "    return res_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(ppi_files,n_gene,method=None,restart_prob=None,embed_dim=None,rand=None,org=None,n_fold=None,k=None,ont_type=None,ont_size1=None,ont_size2=None,n_cluster=None):\n",
    "    ''' \n",
    "    parameters:\n",
    "    - ppi_files: list of str, list of file paths to ppi networks\n",
    "    - n_gene: int, number of genes\n",
    "    - method: list of str, one or more of Mashup, REPEL\n",
    "    - restart_prob: float, RWR restart probability\n",
    "    - embed_dim: int, number of dimension\n",
    "    - rand: int, random split\n",
    "    - org: str, \"yeast\" or \"Ecoli\" \n",
    "    - n_fold: int, total number of folds\n",
    "    - k: int, number of nearest neighbors to be considered\n",
    "    - ont_type: str, bp or mf or cc\n",
    "    - ont_size1: int, 11, 31, 101\n",
    "    - ont_size2: int, 30, 100, 300\n",
    "    - n_cluster: int, number of random augmented nodes\n",
    "    output:\n",
    "    - performance_dict: a dictionary contains list of performances for all methods\n",
    "    '''\n",
    "\n",
    "    performance_dict = {}\n",
    "\n",
    "    for m in method:\n",
    "        m_acc = m + \"_acc\"\n",
    "        m_f1 = m + \"_f1\"\n",
    "        m_auprc = m + \"_auprc\"\n",
    "        performance_dict[m_acc] = []\n",
    "        performance_dict[m_f1] = []\n",
    "        performance_dict[m_auprc] = []\n",
    "        for i in range(n_fold):\n",
    "            print(\"fold: \", i+1)\n",
    "            train_anno, test_anno = load_train_test_anno(rand,i+1,org,ont_type,ont_size1,ont_size2)\n",
    "            if m == \"Mashup\":\n",
    "                print(\"Mashup\")\n",
    "                nets = load_all_nets(ppi_files,n_gene)\n",
    "                walks = compute_rwr_original_sparse(ppi_files,restart_prob,n_gene,nets)\n",
    "                x = svd_embed_sparse_func(walks, n_gene, embed_dim)\n",
    "                dist_mat, knn = get_knn_ind(x,train_anno)\n",
    "                scores, _, _ = majority_vote(dist_mat, knn, train_anno, test_anno,k)\n",
    "                acc,_ = acc_top1_pred(scores, test_anno)\n",
    "                f1, auprc = f1_auprc_pred(scores, test_anno,3)\n",
    "                performance_dict[m_acc].append(acc)\n",
    "                performance_dict[m_f1].append(f1)\n",
    "                performance_dict[m_auprc].append(auprc)\n",
    "            elif m == \"REPEL\":\n",
    "                print(\"REPEL\")\n",
    "                nets = load_all_nets(ppi_files,n_gene)\n",
    "                rand_cluster = random_split_vector(train_anno,n_gene, n_cluster,seed=None)\n",
    "                rand_graph = augment_graph(nets, n_gene, rand_cluster, 1, -1)\n",
    "                rand_rwr_res = augmented_RWR(rand_graph, restart_prob)\n",
    "                mat_rand_x = augmented_SVD_with_cannolink(rand_rwr_res, embed_dim)\n",
    "                dist_mat, knn = get_knn_ind(mat_rand_x,train_anno)\n",
    "                scores, _,_ = majority_vote(dist_mat, knn, train_anno, test_anno,k, weighted=True)\n",
    "                acc,_ = acc_top1_pred(scores, test_anno)\n",
    "                f1, auprc = f1_auprc_pred(scores, test_anno,3)\n",
    "                performance_dict[m_acc].append(acc)\n",
    "                performance_dict[m_f1].append(f1)\n",
    "                performance_dict[m_auprc].append(auprc)\n",
    "\n",
    "            else:\n",
    "                print(\"Haven't implemented yet\")\n",
    "                return\n",
    "    return performance_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOR TESTING\n",
    "\n",
    "\n",
    "def test_get_embed(ppi_files,n_gene,method=None,restart_prob=None,embed_dim=None,rand=None,org=None,n_fold=None,k=None,ont_type=None,ont_size1=None,ont_size2=None,n_cluster=None):\n",
    "    ''' \n",
    "    parameters:\n",
    "    - ppi_files: list of str, list of file paths to ppi networks\n",
    "    - n_gene: int, number of genes\n",
    "    - method: list of str, one or more of Mashup, REPEL\n",
    "    - restart_prob: float, RWR restart probability\n",
    "    - embed_dim: int, number of dimension\n",
    "    - rand: int, random split\n",
    "    - org: str, \"yeast\" or \"Ecoli\" \n",
    "    - n_fold: int, total number of folds\n",
    "    - k: int, number of nearest neighbors to be considered\n",
    "    - ont_type: str, bp or mf or cc\n",
    "    - ont_size1: int, 11, 31, 101\n",
    "    - ont_size2: int, 30, 100, 300\n",
    "    - n_cluster: int, number of random augmented nodes\n",
    "    output:\n",
    "    - one embedding for first fold, and the distance and knn matrices\n",
    "    '''\n",
    "\n",
    "    performance_dict = {}\n",
    "\n",
    "    for m in method:\n",
    "        m_acc = m + \"_acc\"\n",
    "        m_f1 = m + \"_f1\"\n",
    "        m_auprc = m + \"_auprc\"\n",
    "        performance_dict[m_acc] = []\n",
    "        performance_dict[m_f1] = []\n",
    "        performance_dict[m_auprc] = []\n",
    "        for i in range(n_fold):\n",
    "            print(\"fold: \", i+1)\n",
    "            train_anno, test_anno = load_train_test_anno(rand,i+1,org,ont_type,ont_size1,ont_size2)\n",
    "            if m == \"Mashup\":\n",
    "                print(\"Mashup\")\n",
    "                # nets = load_all_nets(ppi_files,n_gene)\n",
    "                # walks = compute_rwr_original_sparse(ppi_files,restart_prob,n_gene,nets)\n",
    "                # x = svd_embed_sparse_func(walks, n_gene, embed_dim)\n",
    "                # dist_mat, knn = get_knn_ind(x,train_anno)\n",
    "                # scores, _, _ = majority_vote(dist_mat, knn, train_anno, test_anno,k)\n",
    "                # acc,_ = acc_top1_pred(scores, test_anno)\n",
    "                # f1, auprc = f1_auprc_pred(scores, test_anno,3)\n",
    "                # performance_dict[m_acc].append(acc)\n",
    "                # performance_dict[m_f1].append(f1)\n",
    "                # performance_dict[m_auprc].append(auprc)\n",
    "            elif m == \"REPEL\":\n",
    "                print(\"REPEL\")\n",
    "                nets = load_all_nets(ppi_files,n_gene)\n",
    "                rand_cluster = random_split_vector(train_anno,n_gene, n_cluster,seed=None)\n",
    "                rand_graph = augment_graph(nets, n_gene, rand_cluster, 1, -1)\n",
    "                rand_rwr_res = augmented_RWR(rand_graph, restart_prob)\n",
    "                mat_rand_x = augmented_SVD_with_cannolink(rand_rwr_res, embed_dim)\n",
    "                dist_mat, knn = get_knn_ind(mat_rand_x,train_anno)\n",
    "                # scores, _,_ = majority_vote(dist_mat, knn, train_anno, test_anno,k, weighted=True)\n",
    "                # acc,_ = acc_top1_pred(scores, test_anno)\n",
    "                # f1, auprc = f1_auprc_pred(scores, test_anno,3)\n",
    "                # performance_dict[m_acc].append(acc)\n",
    "                # performance_dict[m_f1].append(f1)\n",
    "                # performance_dict[m_auprc].append(auprc)\n",
    "                return mat_rand_x, dist_mat, knn, train_anno, test_anno\n",
    "\n",
    "            else:\n",
    "                print(\"Haven't implemented yet\")\n",
    "                return\n",
    "    # return performance_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(performance_dict,rand,org,ont_type,ont_size1,ont_size2,save_path=None):\n",
    "    with open(save_path,\"a\") as f:\n",
    "        tmp = org + \" \" + \"rand \" + str(rand) + \" \" + ont_type + \" \" + str(ont_size1) + \" \" + str(ont_size2) \n",
    "        f.write(tmp)\n",
    "        f.write(\"\\n\")\n",
    "        for k, v in performance_dict.items():\n",
    "            f.write(k)\n",
    "            f.write(\" \")\n",
    "            for i in v:\n",
    "                f.write(f\"{i:.4f}\")\n",
    "                f.write(\" \")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/networks/yeast/yeast_string_neighborhood_adjacency.txt', 'data/networks/yeast/yeast_string_fusion_adjacency.txt', 'data/networks/yeast/yeast_string_cooccurence_adjacency.txt', 'data/networks/yeast/yeast_string_coexpression_adjacency.txt', 'data/networks/yeast/yeast_string_experimental_adjacency.txt', 'data/networks/yeast/yeast_string_database_adjacency.txt']\n",
      "6400\n"
     ]
    }
   ],
   "source": [
    "# yeast\n",
    "string_nets = ['neighborhood', 'fusion', 'cooccurence', 'coexpression', 'experimental', 'database']\n",
    "ppi_files = []\n",
    "for net in string_nets:\n",
    "    tmp = 'data/networks/yeast/yeast_string_'+net+'_adjacency.txt'\n",
    "    ppi_files.append(tmp)\n",
    "\n",
    "all_genes = pd.read_csv(\"data/annotations/yeast/go_yeast_ref_genes.txt\",header=None)\n",
    "all_genes = list(all_genes.iloc[:,0].values)\n",
    "n_gene = len(all_genes)\n",
    "print(ppi_files)\n",
    "print(n_gene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand:  3\n",
      "fold:  1\n",
      "REPEL\n"
     ]
    }
   ],
   "source": [
    "## FOR TESTING\n",
    "\n",
    "\n",
    "method=['REPEL']\n",
    "restart_prob=0.5\n",
    "embed_dim=400\n",
    "org=\"yeast\"\n",
    "n_fold=5\n",
    "k=10\n",
    "ont_type_list=[\"bp\"]\n",
    "ont_size1_list=[31]\n",
    "ont_size2_list=[100]\n",
    "n_cluster=15\n",
    "for rand in [3]:\n",
    "    print(\"rand: \",rand)\n",
    "    ont_size1 = ont_size1_list[0]\n",
    "    ont_size2 = ont_size2_list[0]\n",
    "    mat_rand_x, dist_mat, knn_mat,train_anno, test_anno = test_get_embed(ppi_files,n_gene,method=method,restart_prob=restart_prob,embed_dim=embed_dim,rand=rand,org=org,n_fold=n_fold,k=k,ont_type=ont_type_list[0],ont_size1=ont_size1,ont_size2=ont_size2,n_cluster=n_cluster)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplest version of cascading: we do majority vote, but we allow several rounds to label unlabeled nodes based on their neighbors. As soon as\n",
    "# it gets a pseudo-label, it is allowed to vote.\n",
    "# Once a label is gained, it cannot be lost\n",
    "# Does not search for 20 most similar that are LABELED, it just takes the 20 most similar\n",
    "# Does not give pseudolabels to originally labeled nodes\n",
    "\n",
    "def cascade_vote_unweighted_CC(dist_mat, knn_mat, train_anno, test_anno,k, max_runs = 6, weighted = True, cascade_k = 15, label_threshold = 0.7, min_voting_nodes_cascade = 8, relabel_original = False):\n",
    "    '''\n",
    "    parameters:\n",
    "    - dist_mat: n_gene x n_gene\n",
    "    - knn_mat: ngene x (ngene-1), sorted labels\n",
    "    - train_anno: n_label x n_gene\n",
    "    - test_anno: n_label x n_gene\n",
    "    - k: number of nearest neighbors\n",
    "    - weighted: whether the FINAL VOTE (not cascading) is weighted\n",
    "    - max_runs: how many iterations to do\n",
    "    - label_threshold: how many votes until it has a pseudo-label\n",
    "    - cascade_k: you can set a different k for cascading\n",
    "    - min_voting_nodes_cascade: don't cascade if too few nodes (otherwise, all with one labeled neighbor will cascade)\n",
    "    - relabel_original: allow adding labels to original\n",
    "    \n",
    "    output:\n",
    "    - final_scores: n_label x n_test, normalized scores of each label\n",
    "    - num_voters: vector of numbers of voting nodes\n",
    "    '''\n",
    "    train_idx = np.where(sum(train_anno)>0)[0]\n",
    "    test_idx = np.where(sum(test_anno)>0)[0]\n",
    "    final_scores = np.zeros((train_anno.shape[0],len(test_idx)))\n",
    "    num_voters = []\n",
    "    updated_voters = []\n",
    "\n",
    "    # All labels, including pseudolabels. start off with only the train labels\n",
    "    all_labels = train_anno.copy()\n",
    "\n",
    "    # All the nodes that we don't have a label for (not just the test labels!)\n",
    "    original_unlabeled_nodes = np.where(sum(train_anno) == 0)[0]\n",
    "    curr_unlabeled_nodes = set(original_unlabeled_nodes)\n",
    "\n",
    "    # Reset to all possible nodes\n",
    "    if relabel_original :\n",
    "        original_unlabeled_nodes=  np.concatenate((original_unlabeled_nodes, train_idx))\n",
    "\n",
    "\n",
    "\n",
    "    # These runs will create pseudolabels\n",
    "    for run in range(max_runs - 1):\n",
    "        # print(f\"Remaining unlabeled nodes: {len(curr_unlabeled_nodes)}\")\n",
    "        # Each new label gets put here so we don't label things in the middle of iterations\n",
    "        new_labels = []\n",
    "        for unlabeled_node in original_unlabeled_nodes:\n",
    "            nn = knn_mat[unlabeled_node, :cascade_k]\n",
    "            # Take any pseudolabeled nodes too!\n",
    "            nn_labeled = [node for node in nn if node not in curr_unlabeled_nodes]\n",
    "            if len(nn_labeled) >= min_voting_nodes_cascade :\n",
    "                votes = np.array(all_labels[:,nn_labeled])\n",
    "                scores = np.sum(votes,axis=1)\n",
    "                scores = scores / len(nn_labeled)\n",
    "                for new_label in np.where(scores > label_threshold)[0] :\n",
    "                    if all_labels[new_label][unlabeled_node] == 0 :\n",
    "                        new_labels.append((new_label, unlabeled_node))\n",
    "        \n",
    "        for tup in new_labels :\n",
    "            all_labels[tup[0]][tup[1]] = 1\n",
    "            if tup[1] in curr_unlabeled_nodes :\n",
    "                curr_unlabeled_nodes.remove(tup[1])\n",
    "\n",
    "\n",
    "    # Finally, we make final predictions\n",
    "\n",
    "    for index, i in enumerate(test_idx):\n",
    "        nn = knn_mat[i,:k]\n",
    "        nn_labeled = np.array([node for node in nn if node not in curr_unlabeled_nodes])\n",
    "\n",
    "        if len(nn_labeled) == 0: # if within the first k neighbors, no neighbor is labeled, then use the nearest neighbor with label\n",
    "                voting_node = knn_mat[i,:][np.isin(knn_mat[i,:], train_idx)][0]\n",
    "                scores = np.array(train_anno[:,voting_node])\n",
    "                scores = scores / sum(scores)\n",
    "                num_voters.append(len(nn_labeled))\n",
    "                tmp = [voting_node]\n",
    "                updated_voters.append(tmp)\n",
    "        else :\n",
    "\n",
    "            if weighted: \n",
    "                d = dist_mat[i,nn_labeled]\n",
    "                d = d[np.nonzero(d)]\n",
    "                votes = np.array(all_labels[:,nn_labeled[np.nonzero(d)]])\n",
    "                tmp = nn_labeled[np.nonzero(d)]\n",
    "                updated_voters.append(tmp)\n",
    "                num_voters.append(len(d))\n",
    "                if len(d) == 0:\n",
    "                    c += 1\n",
    "                    voting_node = np.random.choice(train_idx)\n",
    "                    scores = np.array(train_anno[:,voting_node])\n",
    "                    scores = scores / sum(scores)\n",
    "                else:\n",
    "                    weights = 1 / d\n",
    "                    scores = votes @ weights.T\n",
    "                    scores = scores / sum(scores)\n",
    "        \n",
    "            else:\n",
    "                votes = np.array(all_labels[:,nn_labeled])\n",
    "                \n",
    "                num_voters.append(len(nn_labeled))\n",
    "                updated_voters.append(nn_labeled)\n",
    "                scores = np.sum(votes,axis=1)\n",
    "                scores = scores / sum(scores)\n",
    "            \n",
    "        final_scores[:,index] = np.squeeze(scores)\n",
    "\n",
    "\n",
    "    return final_scores, num_voters,updated_voters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted KNN (Original) Accuracy & F1 acc, auprc\n",
      "0.525062656641604\n",
      "(0.26671732522796354, 0.3211263665152079)\n",
      "Unweighted KNN (Original) Accuracy & F1 acc, auprc\n",
      "0.5100250626566416\n",
      "(0.25987841945288753, 0.3130670489396711)\n"
     ]
    }
   ],
   "source": [
    "final_scores_knn_uw, num_voters_knn_uw,updated_voters_knn_uw = majority_vote(dist_mat,knn_mat,train_anno,test_anno,10,False)\n",
    "final_scores_knn, num_voters_knn,updated_voters_knn = majority_vote(dist_mat,knn_mat,train_anno,test_anno,10,True)\n",
    "print(\"Weighted KNN (Original) Accuracy & F1 acc, auprc\")\n",
    "print(acc_top1_pred(final_scores_knn,test_anno)[0])\n",
    "print(f1_auprc_pred(final_scores_knn, test_anno, 3))\n",
    "print(\"Unweighted KNN (Original) Accuracy & F1 acc, auprc\")\n",
    "print(acc_top1_pred(final_scores_knn_uw,test_anno)[0])\n",
    "print(f1_auprc_pred(final_scores_knn_uw, test_anno, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted Cascading Accuracy & F1 acc, auprc\n",
      "0.5162907268170426\n",
      "Diff: 0.006266\n",
      "(0.26165146909827763, 0.31515650164443987)\n",
      "Diffs: 0.001773, 0.002089\n",
      "Weighted Cascading Accuracy & F1 acc, auprc\n",
      "0.5300751879699248\n",
      "Diff: 0.005013\n",
      "(0.26773049645390073, 0.32232033948936145)\n",
      "Diffs: 0.001013, 0.001194\n"
     ]
    }
   ],
   "source": [
    "cascade_k = 15\n",
    "label_threshold = 0.7\n",
    "min_voting_nodes_cascade = 8\n",
    "max_runs = 6\n",
    "k = 10\n",
    "relabel_originals = False\n",
    "final_cascade_plain, num_voters_cascade_plain, updated_voters_cascade_plain = cascade_vote_unweighted_CC(dist_mat, knn_mat, train_anno, test_anno, k, max_runs, False ,cascade_k, label_threshold, min_voting_nodes_cascade, relabel_originals)\n",
    "print(\"Unweighted Cascading Accuracy & F1 acc, auprc\")\n",
    "print(acc_top1_pred(final_cascade_plain,test_anno)[0])\n",
    "print(f\"Diff: {np.round(-acc_top1_pred(final_scores_knn_uw,test_anno)[0] + acc_top1_pred(final_cascade_plain,test_anno)[0], 6)}\")\n",
    "print(f1_auprc_pred(final_cascade_plain,test_anno, 3))\n",
    "print(f\"Diffs: {np.round(f1_auprc_pred(final_cascade_plain,test_anno, 3)[0] - f1_auprc_pred(final_scores_knn_uw, test_anno, 3)[0], 6)}, {np.round(f1_auprc_pred(final_cascade_plain,test_anno, 3)[1] - f1_auprc_pred(final_scores_knn_uw, test_anno, 3)[1], 6)}\")\n",
    "\n",
    "\n",
    "\n",
    "final_cascade_plain, num_voters_cascade_plain, updated_voters_cascade_plain = cascade_vote_unweighted_CC(dist_mat, knn_mat, train_anno, test_anno, k, max_runs, True ,cascade_k, label_threshold, min_voting_nodes_cascade)\n",
    "print(\"Weighted Cascading Accuracy & F1 acc, auprc\")\n",
    "print(acc_top1_pred(final_cascade_plain,test_anno)[0])\n",
    "print(f\"Diff: {np.round(-acc_top1_pred(final_scores_knn,test_anno)[0] + acc_top1_pred(final_cascade_plain,test_anno)[0], 6)}\")\n",
    "print(f1_auprc_pred(final_cascade_plain,test_anno, 3))\n",
    "print(f\"Diffs: {np.round(f1_auprc_pred(final_cascade_plain,test_anno, 3)[0] - f1_auprc_pred(final_scores_knn, test_anno, 3)[0], 6)}, {np.round(f1_auprc_pred(final_cascade_plain,test_anno, 3)[1] - f1_auprc_pred(final_scores_knn, test_anno, 3)[1], 6)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand:  1\n",
      "ont_type\n",
      "fold:  1\n",
      "REPEL\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.04839128 ... 0.         0.         0.        ]]\n",
      "fold:  2\n",
      "REPEL\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "fold:  3\n",
      "REPEL\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m ont_size1 = ont_size1_list[c]\n\u001b[32m     17\u001b[39m ont_size2 = ont_size2_list[c]\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m performance_dict = \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppi_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_gene\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrestart_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrestart_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m,\u001b[49m\u001b[43morg\u001b[49m\u001b[43m=\u001b[49m\u001b[43morg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_fold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mont_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mont_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43mont_size1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mont_size1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mont_size2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mont_size2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_cluster\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_cluster\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m write_log(performance_dict,rand,org,ont_type,ont_size1,ont_size2,save_path=\u001b[33m\"\u001b[39m\u001b[33myeast_result_log.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(performance_dict)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m(ppi_files, n_gene, method, restart_prob, embed_dim, rand, org, n_fold, k, ont_type, ont_size1, ont_size2, n_cluster)\u001b[39m\n\u001b[32m     48\u001b[39m rand_cluster = random_split_vector(train_anno,n_gene, n_cluster,seed=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     49\u001b[39m rand_graph = augment_graph(nets, n_gene, rand_cluster, \u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m rand_rwr_res = \u001b[43maugmented_RWR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrand_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestart_prob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m mat_rand_x = augmented_SVD_with_cannolink(rand_rwr_res, embed_dim)\n\u001b[32m     52\u001b[39m dist_mat, knn = get_knn_ind(mat_rand_x,train_anno)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36maugmented_RWR\u001b[39m\u001b[34m(augmented_nets, restart_prob)\u001b[39m\n\u001b[32m     11\u001b[39m d = np.absolute(A) @ e\n\u001b[32m     12\u001b[39m L = np.diag(d) - (\u001b[32m1\u001b[39m-restart_prob)*A\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m L_inv = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m W = restart_prob*(np.diag(d) @ L_inv)\n\u001b[32m     15\u001b[39m augmented_walks[i,:,:] = W\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kmyut\\anaconda3\\envs\\capstone\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:553\u001b[39m, in \u001b[36m_unary_dispatcher\u001b[39m\u001b[34m(a)\u001b[39m\n\u001b[32m    548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ia.reshape(*invshape)\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m# Matrix inversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_unary_dispatcher\u001b[39m(a):\n\u001b[32m    554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a,)\n\u001b[32m    557\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_unary_dispatcher)\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minv\u001b[39m(a):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "method=['REPEL']\n",
    "restart_prob=0.5\n",
    "embed_dim=400\n",
    "org=\"yeast\"\n",
    "n_fold=5\n",
    "k=10\n",
    "ont_type_list=[\"bp\",\"mf\",\"cc\"]\n",
    "ont_size1_list=[31]\n",
    "ont_size2_list=[100]\n",
    "n_cluster=15\n",
    "for rand in range(1,6):\n",
    "    print(\"rand: \",rand)\n",
    "    for ont_type in ont_type_list:\n",
    "        print(\"ont_type\")\n",
    "        for c in range(3):\n",
    "            ont_size1 = ont_size1_list[c]\n",
    "            ont_size2 = ont_size2_list[c]\n",
    "            performance_dict = run_pipeline(ppi_files,n_gene,method=method,restart_prob=restart_prob,embed_dim=embed_dim,rand=rand,org=org,n_fold=n_fold,k=k,ont_type=ont_type,ont_size1=ont_size1,ont_size2=ont_size2,n_cluster=n_cluster)\n",
    "            write_log(performance_dict,rand,org,ont_type,ont_size1,ont_size2,save_path=\"yeast_result_log.txt\")\n",
    "            print(performance_dict)\n",
    "    #         break\n",
    "    #     break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For E. coli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/networks/Ecoli/ecoli_string_neighborhood_adjacency.txt', 'data/networks/Ecoli/ecoli_string_fusion_adjacency.txt', 'data/networks/Ecoli/ecoli_string_cooccurence_adjacency.txt', 'data/networks/Ecoli/ecoli_string_coexpression_adjacency.txt', 'data/networks/Ecoli/ecoli_string_experimental_adjacency.txt', 'data/networks/Ecoli/ecoli_string_database_adjacency.txt']\n",
      "4140\n"
     ]
    }
   ],
   "source": [
    "# for E.coli\n",
    "string_nets = ['neighborhood', 'fusion', 'cooccurence', 'coexpression', 'experimental', 'database']\n",
    "ppi_files = []\n",
    "for net in string_nets:\n",
    "    tmp = 'data/networks/Ecoli/ecoli_string_'+net+'_adjacency.txt'\n",
    "    ppi_files.append(tmp)\n",
    "\n",
    "all_genes = pd.read_csv(\"data/annotations/Ecoli/ecoli_ref_genes.txt\",header=None,sep='\\t')\n",
    "all_genes = list(all_genes.iloc[:,1].values)\n",
    "n_gene = len(all_genes)\n",
    "print(ppi_files)\n",
    "print(n_gene)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand:  1\n",
      "ont_type\n",
      "fold:  1\n",
      "Mashup\n",
      "fold:  2\n",
      "Mashup\n",
      "fold:  3\n",
      "Mashup\n",
      "fold:  4\n",
      "Mashup\n",
      "fold:  5\n",
      "Mashup\n",
      "fold:  1\n",
      "REPEL\n",
      "fold:  2\n",
      "REPEL\n",
      "fold:  3\n",
      "REPEL\n",
      "fold:  4\n",
      "REPEL\n",
      "fold:  5\n",
      "REPEL\n",
      "{'Mashup_acc': [np.float64(0.4166666666666667), np.float64(0.47474747474747475), np.float64(0.45569620253164556), np.float64(0.5012658227848101), np.float64(0.4531645569620253)], 'Mashup_f1': [0.3101030927835052, 0.3415821501014199, 0.3191407748369774, 0.37645174209050863, 0.33726127590410404], 'Mashup_auprc': [np.float64(0.31310573851331064), np.float64(0.34488730032756837), np.float64(0.32516911124506065), np.float64(0.38024772694230863), np.float64(0.340606303100847)], 'REPEL_acc': [np.float64(0.5984848484848485), np.float64(0.6136363636363636), np.float64(0.6278481012658228), np.float64(0.6658227848101266), np.float64(0.620253164556962)], 'REPEL_f1': [0.43381443298969075, 0.4486815415821501, 0.4158036056770234, 0.47577092511013214, 0.44697277529459567], 'REPEL_auprc': [np.float64(0.4363665672780467), np.float64(0.45168557229159334), np.float64(0.4222155247471703), np.float64(0.4794092561555513), np.float64(0.4500159353716753)]}\n"
     ]
    }
   ],
   "source": [
    "method=['Mashup','REPEL']\n",
    "restart_prob=0.5\n",
    "embed_dim=400\n",
    "org=\"Ecoli\"\n",
    "n_fold=5\n",
    "k=10\n",
    "ont_type_list=[\"bp\",\"mf\",\"cc\"]\n",
    "ont_size1_list=[11,31,101]\n",
    "ont_size2_list=[30,100,300]\n",
    "n_cluster=15\n",
    "for rand in range(1,6):\n",
    "    print(\"rand: \",rand)\n",
    "    for ont_type in ont_type_list:\n",
    "        print(\"ont_type\")\n",
    "        for c in range(3):\n",
    "            ont_size1 = ont_size1_list[c]\n",
    "            ont_size2 = ont_size2_list[c]\n",
    "            performance_dict = run_pipeline(ppi_files,n_gene,method=method,restart_prob=restart_prob,embed_dim=embed_dim,rand=rand,org=org,n_fold=n_fold,k=k,ont_type=ont_type,ont_size1=ont_size1,ont_size2=ont_size2,n_cluster=n_cluster)\n",
    "            write_log(performance_dict,rand,org,ont_type,ont_size1,ont_size2,save_path=\"Ecoli_result_log.txt\")\n",
    "            print(performance_dict)\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
